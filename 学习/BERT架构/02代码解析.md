1.bert_data1.py，加载数据文件
`load_from_disk()` 是 Hugging Face `datasets` 库提供的方法，用于从本地磁盘加载 **已保存的 Arrow 格式数据集**（通常由 `Dataset.save_to_disk()` 保存）。
参数`split` 是数据集的划分标识
```python
from torch.utils.data import Dataset
from datasets import load_from_disk
class Mydataset(Dataset):
    #初始化数据
    def __init__(self,split):
        #从磁盘加载数据
        self.dataset = load_from_disk(r"D:\PycharmProjects\demo_15\data\ChnSentiCorp")
        if split =="train":
            self.dataset = self.dataset["train"]
        elif split == "validation":
            self.dataset = self.dataset["validation"]
        elif split=="test":
            self.dataset = self.dataset["test"]
        else:
            print("数据集名称错误！")
    #获取数据集大小
    def __len__(self):
        return len(self.dataset)
    #对数据做定制化处理
    def __getitem__(self, item):
        text = self.dataset[item]["text"]
        label = self.dataset[item]["label"]
        return text,label
if __name__ == '__main__':
    dataset = Mydataset("validation")
    for data in dataset:
        print(data)
        ```
 2.加载模型bert_net1.py
 注意这个token = BertTokenizer.from_pretrained()这个函数里面要放你下载的模型的位置。或者在线模型名，会从Hugging Face服务器上下载。其他参数，如`local_files_only=True`表示强制使用本地缓存，避免联网。

 这里为什么要使用BertModel?
 是因为`transformers` 库采用面向对象设计，每个预训练模型（如 BERT、GPT-2）都有对应的 **专属类**（如 `BertModel`、`GPT2Model`）。不同模型的结构差异巨大（BERT 是双向Transformer编码器，GPT 是单向解码器），通过类名明确指定加载的模型类型，而且- 类内部实现了该模型特有的前向传播逻辑、注意力机制等。
 当然也可以使用`AutoModel.from_pretrained()`来动态适配不同架构。在明确知道模型类型时，**直接使用 `BertModel` 更推荐**

`self.fc = torch.nn.Linear(768, 2)` 定义了一个**全连接层**（Fully Connected Layer），通常用于模型的最终分类任务。**`fc`** 是 "Fully Connected" 的缩写，中文称**全连接层**或**线性层**。768是打印pretrained看到的模型输出向量是768维，2是因为我们做的是二分类任务
**为什么需要这个层？**
BERT本身是一个**预训练的语言模型**，其原始输出是每个token的上下文向量（形状为 `[batch_size, seq_len, 768]`）。
- 如果要做**句子分类任务**（如情感分析），通常需要：
    1. 获取整个句子的表示（如取 `[CLS]` 标记的向量，或做Pooling）。
    2. 通过 `self.fc` 将768维向量映射到类别数（如2维），再通过Softmax得到概率。

`with torch.no_grad():` 是 PyTorch 中的一个**上下文管理器**（Context Manager），用于**临时禁用梯度计算**，通常在模型推理（预测）或评估阶段或冻结部分参数时使用。

 **`out = self.fc(out.last_hidden_state[:, 0])`**
**功能*：
- **提取[CLS]标记的向量**：  
    `out.last_hidden_state[:, 0]` 从BERT的输出中提取每个样本的 `[CLS]` 标记的768维向量。
    - `last_hidden_state` 形状：`[batch_size, seq_len, 768]`
    - `[:, 0]` 表示取所有样本（`:`）的第一个标记（`0`，即 `[CLS]`），结果形状：`[batch_size, 768]`    
- **全连接层分类**：  
    `self.fc` 是一个 `nn.Linear(768, num_classes)` 层，将768维向量映射到类别数的维度（如二分类时为2）。
    - 输出 `logits`（未归一化的类别分数），形状：`[batch_size, num_classes]` 
**`out = out.softmax(dim=1)`**
**功能**：
- **Softmax归一化**：  
    对 `logits` 在维度 `dim=1`（类别维度）上计算Softmax，将分数转换为概率分布。
    结果每个样本的各类别概率之和为1，形状仍为 `[batch_size, num_classes]`。
 ```python
 #加载预训练模型
from transformers import BertModel
import torch
#定义训练设备
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
pretrained = BertModel.from_pretrained("/data/home/zengxiangxi/xuexi/bertlianxi/huggingface/model_cache/models--bert-base-chinese/snapshots/c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f").to(DEVICE)
print(pretrained)
#定义下游任务模型（将主干网络所提取的特征进行分类）
class Model(torch.nn.Module):
    #模型结构设计
    def __init__(self):
        super().__init__()
        self.fc = torch.nn.Linear(768,2)
    def forward(self,input_ids,attention_mask,token_type_ids):
        #上游任务不参与训练
        with torch.no_grad():
            out = pretrained(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)
        #下游任务参与训练
        out = self.fc(out.last_hidden_state[:,0])
        out = out.softmax(dim=1)
        return out
        ```
 3.进行训练bert_trainer1.py
 注意这个token = BertTokenizer.from_pretrained()这个函数里面要放你下载的模型的位置
 `collate_fn` 的作用是：**将一批原始数据（文本句子和标签）转换为模型可接受的张量格式**。具体包括：
1. 提取文本和标签。
2. 使用分词器（`token`）对文本进行编码（包括截断、填充等）。
3. 返回模型所需的张量（`input_ids`, `attention_mask`, `token_type_ids`, `labels`）。
**`token.batch_encode_plus`**：Hugging Face 分词器的方法，对批量文本进行编码。
- `truncation=True`：超过 `max_length` 的文本会被截断。
- `padding="max_length"`：填充到 `max_length=350`。
- `return_tensors="pt"`：返回 PyTorch 张量（而非列表）。
- `return_length=True`：返回实际长度

**`collate_fn` 是函数对象**：直接传入函数名（无需括号调用）。 **必须匹配输入格式**：假设 `train_dataset[i]` 返回 `(文本, 标签)`，则 `collate_fn` 的 `data` 参数是这些元组的列表。
 ```python
import torch
from bert_data1 import Mydataset
from torch.utils.data import DataLoader
from bert_net1 import Model
from transformers import BertTokenizer,AdamW
#定义训练设备
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
EPOCH = 100
token = BertTokenizer.from_pretrained("/data/home/zengxiangxi/xuexi/bertlianxi/huggingface/model_cache/models--bert-base-chinese/snapshots/c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f")
#自定义函数，对数据进行编码处理
def collate_fn(data):
    sentes = [i[0] for i in data]
    label = [i[1] for i in data]
    #编码
    data = token.batch_encode_plus(
        batch_text_or_text_pairs=sentes,
        truncation=True,
        padding="max_length",
        max_length=350,
        return_tensors="pt",
        return_length=True,
    )
    input_ids = data["input_ids"] # 编码后的词ID张量
    attention_mask = data["attention_mask"]# 注意力掩码（区分真实词和填充符）
    token_type_ids = data["token_type_ids"]# 区分句子A和B（单句任务通常全0）
    labels = torch.LongTensor(label)# 将标签列表转为LongTensor
    return input_ids,attention_mask,token_type_ids,labels
#创建数据集
train_dataset = Mydataset("train")
#创建DataLoader
train_laoder = DataLoader(
    dataset=train_dataset,
    batch_size=32,
    shuffle=True,
    drop_last=True,
    collate_fn=collate_fn
)
if __name__ == '__main__':
    #开始训练
    print(DEVICE)
    model = Model().to(DEVICE)
    optimizer = AdamW(model.parameters(),lr=5e-4)
    loss_func = torch.nn.CrossEntropyLoss()
    model.train()
    for epoch in range(EPOCH):
        for i,(input_ids,attention_mask,token_type_ids,labels) in enumerate(train_laoder):
            #将数据放到DEVICE上
            input_ids, attention_mask, token_type_ids, labels = input_ids.to(DEVICE),\
    attention_mask.to(DEVICE),token_type_ids.to(DEVICE),labels.to(DEVICE)
            #执行前向计算得到输出
            out = model(input_ids, attention_mask, token_type_ids)
            loss = loss_func(out,labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if i%5 ==0:
                out = out.argmax(dim=1)
                acc = (out == labels).sum().item()/len(labels)
                print(epoch,i,loss.item(),acc)
        #保存模型参数
        torch.save(model.state_dict(),f"params/{epoch}bert.pt")
        print(epoch,"参数保存成功！")
        ```
 4.进行测试，bert_run1.py
 batch_text_or_text_pairs参数决定了需要被批量编码的文本数据。
 ```python
import torch
from bert_net1 import Model
from transformers import BertTokenizer
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
names = ["负向评价","正向评价"]
print(DEVICE)
model = Model().to(DEVICE)
token = BertTokenizer.from_pretrained("bert-base-chinese")
def collate_fn(data):
    sentes = []
    sentes.append(data)
    # print(sentes)
    #编码
    data = token.batch_encode_plus(batch_text_or_text_pairs=sentes,
                            truncation=True,
                            padding="max_length",
                            max_length=500,
                            return_tensors="pt",
                            return_length=True)
    input_ids = data["input_ids"]
    attention_mask = data["attention_mask"]
    token_type_ids = data["token_type_ids"]
    # print(input_ids,attention_mask,token_type_ids)
    return input_ids,attention_mask,token_type_ids
def test():
    model.load_state_dict(torch.load("params/2bert.pt"))
    model.eval()
    while True:
        data = input("请输入测试数据(输入'q'退出)：")
        if data == "q":
            print("测试结束")
            break
        input_ids, attention_mask, token_type_ids= collate_fn(data)
        input_ids, attention_mask, token_type_ids = input_ids.to(DEVICE), attention_mask.to(
            DEVICE), token_type_ids.to(DEVICE)
        with torch.no_grad():
            out = model(input_ids,attention_mask,token_type_ids)
            out = out.argmax(dim=1)
            print("模型判定：",names[out],"\n")
if __name__ == '__main__':
    test()
    ```