我们日常接触比较多的是NLP自然语义处理模型，如Qwen、ChatGPT、Llama生成模型，也是单向语言生成模型：根据上个句子来续写下一个句子
BERT（Bidirectional Encoder Representations from Transformers）(基于 Transformer 的双向编码器表示) 是由谷歌在 2018 年提出的预训练语言模型，其核心思想是通过双向 Transformer 编码器学习文本的上下文语义表示，开创了 “预训练 + 微调” 的通用框架。
##### BERT 的技术特点
**1.双向编码（Bidirectional Encoding)**
通过遮蔽语言模型（Masked Language Model, MLM) 强制模型同时关注词语的上下文信息。例如，将句子中 15% 的 Token 替换为` [Mask]`，让模型预测原词，从而学习双向语义关联。
**2.Transformer编码器架构**
完全基于 Transformer 的编码器层构建，利用自注意力机制（Self-Attention）并行处理序列中的每个 Token，捕捉长距离依赖关系，相比循环神经网络（RNN）效率更高、性能更强
**3.下一句预测（Next Sentence Prediction, NSP）**
预训练时额外加入二元分类任务：判断两个句子是否为连续文本。例如，输入 “A [SEP] B”，模型预测 “是” 或 “否”，以增强对句子级语义关联的理解。

##### 预训练阶段
- **数据**：海量无标注文本（如维基百科、BooksCorpus）。
- **目标**：通过 MLM 和 NSP 任务学习通用语义表示。
- **输出**：生成每个 Token 的上下文向量（Contextual Embedding），同一词语在不同语境中会有不同的向量表示（解决一词多义问题）。
##### 微调阶段
- **适配任务**：针对具体任务（如分类、命名实体识别、问答等），在 BERT 预训练模型基础上添加少量特定层，利用标注数据微调模型参数。
- **支持任务类型**：
    - **单文本分类**：情感分析、垃圾邮件检测等。
    - **文本对任务**：自然语言推理（判断两个句子的逻辑关系）、问答系统（根据上下文回答问题）等。
    - **序列标注**：命名实体识别（NER）、分词等。

##### BERT模型参数
- BERT-base模型：L=12，H=768，A=12，参数总理110M
- BERT-large模型：L=24，H=1024，A=16，参数总量340M
- L表示网络的层数，H表示隐藏层的大小，A表示Multi-Head Attention中self-Attention的数量

##### BERT应用场景
BERT是根据我们输入的话做一个特征提取，输出label,score。我们需要基于我们的BERT设计下游的NLP任务，来处理它输出的结果。
- 句子分类，如垃圾邮件分类
- 情感分析，判断评价是正面还是负面
- 问答系统，回答用户提出的问题
- 序列标注任务，如命名实体识别
- BERT的预训练模型可以通过微调应用于各种下游NLP任务

##### 基于BERT的微调任务
- 单文本分类任务。如判断这个句子是正面还是负面
- 语句分类任务。如判断问题与答案是否匹配、两个句子是否表示同一个意思
- 序列标注任务。命名实体识别、中文分词

##### BERT模型的下载
- 使用hugging face平台，要科学上网。默认里面的模型是开源的
- modelscope魔塔社区
tob一般是模型本地私有化部署

1.访问hugging face，注册登录
2.安装rust环境。https://www.rust-lang.org/zh-CN/tools/install
下载好，就双击运行，其中按enter键。然后完成
![[Pasted image 20250608093734.png]]
3.安装hugging face库
```
pip install transformers datasets tokenizers
```
4.运行代码，安装模型(要科学上网).文件名download_model.py
```python
from transformers import AutoModel,AutoTokenizer
#下载的模型名称
model_name="bert-base-chinese"
#指定模型保存路径，若不指定路径，默认保存在当前用户目录下C:\Users\Administrator\.cache\huggingface\hub
cache_dir="E:\huggingface\model_cache"
#下载并加载模型和分词器到指定文件夹
model=AutoModel.from_pretrained(model_name,cache_dir=cache_dir)
tokenizer=AutoTokenizer.from_pretrained(model_name,cache_dir=cache_dir)
```
![[Pasted image 20250608155415.png]]
5.准备数据集
训练集、验证集、测试集。是否符合项目需求(模型训练需求)
前期数据:原始数据(语料库) 如这件商品不错 0 物流太慢 1
一般使用Pytorch来处理数据集，让模型开源识别的数据
6.下游任务模型设计
在微调BERT模型之前，需要设计一个适应情感分析任务的下游模型结构，通常包括一个或多个全连接层，用于将BERT输出的特征向量转换为分类结果。
![[Pasted image 20250608165447.png]]
AutoModel是下载模型，BertModel是加载模型。
设计了一个一层全连接层
7.训练
硬件配置：如Qwen7B:推理：需要显存24G以上(也可以直接量化部署)，如两张24G
                 微调：半精度QLoRA
我们这里是有9600条数据，建议在20-50轮训练
打印准确度，就是有监督微调sft
运行
8.模型评估
很多时候边训练，边验证：有效保证模型的训练效果
test.py
![[Pasted image 20250608202537.png]]
![[Pasted image 20250608202559.png]]
![[Pasted image 20250608203433.png]]
![[Pasted image 20250608203456.png]]
run.py
![[Pasted image 20250608203849.png]]