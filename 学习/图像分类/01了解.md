我们的训练集和验证集里面的数据是一个个文件夹，每个文件夹是一个类别，里面是一些图片。即使我们的数据集很少，但是还是可以训练出好的模型的，如做数据增强。
```python
pip install torchvision
```
我们从该库里面导入的transforms包是用来做数据增强的
models包来使用现成的，如常用的resnet
datasets包来加载数据和标签

`.Compose` 是指按照下面的顺序依次执行
`.Resize` 重新定义输入数据的大小，因为我们的数据集图片的长宽不一致，对于卷积神经网络来说这是需要把它们的尺寸定义成一样的。里面的数值写小一点的比较好，这样迭代速度快，虽然会丢失一些信息，如果值过大，速度很慢，可能就是跑不起来。这里选择96，是因为这个计算起来比较快。平时我们用CPU去跑，可能得64。

**数据增强** 就是让我们的数据具有多样性，对原有数据进行更丰富的变换，如平移、旋转、缩放、翻转、裁剪等。得到的图形是没有保存在本地的，一般预存在内存或显存中。

因为我们的输入数据比较小，才64，所以batch_size=128比较大
ImageFolder就是以线程指定文件夹的方式加载数据集
`class_names` 的索引位置是从1开始排的，如1 10 100 101 102 11 12 这样排的

迁移学习：类似对已经训练好的模型进行微调。刚开始都用现成的，然后一步一步修改
我们直接使用resnet模型，而且用它训练好的权重参数来做初始化
对于数据量比较少的，我们可以冻住模型的一些网络层，就是参数不更新，只修改一部分参数。输出层是要自己训练的.输出层可能是其他维度的，我们要修改为我们的分类数
对于数据量是中级的，我们可以冻住一小部分的网络层。
对于数据量非常大的，可以全部都进行更新。
`avgpool:AdaptiveAvgPool2d(output_size=(1,1))` 全局平均值池化 如14×14×512变为1×512

学习率衰减策略：`optim.lr_scheduler.StepLR(optimizer_ft,step_size=10,gammga=0.1)` 使用StepLR 方法，告诉我什么时候衰减就衰减一下。第一个参数是传递优化器.第二个参数是学习率调用十次，衰减一次。

**训练模块**
`def train_model(model,dataloaders,criterion,optimzer,num_epochs=25,filename='best.pt'):`
传入模型，损失函数，优化器，迭代次数，保存的模型文件名

##### 测试数据预处理
测试数据处理方法需要跟训练一致；crop操作的目的是保证输入的大小是一致的；标准化操作也是必须的，用跟训练数据相同的mean和std，但是需要注意一点训练数据是在0-1上进行标准化，所以测试数据也需要先归一化；Pytorch中颜色通道是第一个维度，跟很多工具包都不一样，需要转换

验证集的准确率达到85%才算是好的，要是54%那样，那可能就是学习率设置过大，一般设置为le-5.

流程总结：导入库包，定义好数据文件的位置地址，因为数据量少，所以我们先进行数据增强，加载花名的文件夹，做一个字典映射。加载models里面提供的模型resnet,选择18层的神经网络，定义函数，对我们参数进行冻结。重写模型输出层。保存需要训练的参数列表(即我们重写部分的参数)，设置优化器，定义学习率衰减策略，定义损失函数，因为我们是分类任务，所以使用交叉熵损失函数。定义训练模块的函数：把模型放入设备，定义准确率为0，提取优化器的学习率，初始化模型权重，写for循环，对于每一个打印，再写一个for循环训练集和验证集，进行模型转换，再写一个for循环，对批次数据，把图像和标签输入到设备，梯度清零，把输入放到模型里面，计算损失函数，得到预测最大值，对训练集数据进行反向传播，参数更新。然后计算损失值和准确率。把验证集最好的准确率那次的模型参数复制到模型里面(模型参数权重更新)。 接着，我们取消参数冻结，训练所有层，定义优化器、衰减策略、损失函数，加载之前训练好的权重参数。取测试数据，来得到概率最大的类别id，然后展示预测结果。