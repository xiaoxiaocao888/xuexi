1.自定义数据集加载，加载鸢尾花数据集，并将其划分为训练集和测试集
2.`nn.Module`构造神经网络模型，对于这个问题，可以使用一个简单的多层感知器(MLP)模型。在输出层，可以使用Softmax激活函数将输出转换为概率分布
3.模型训练
- 选择一个适当的损失函数，如交叉熵损失(Cross-Entropy Loss)，用于衡量模型预测与实际标签之间的差异。
- 选择一个优化算法，例如随机梯度下降(SGD)或Adam等，用于更新神经网络的权重以最小化损失函数
- 设置训练参数，如批量大小(Batch Size)、学习率(Learning Rate)和训练轮数(Epochs)等
- 使用训练集数据进行训练，不断更新神经网络权重以最小化损失
4.模型评估
- 使用测试集数据对训练好的神经网络模型进行评估，计算分类准确率等指标
- 如果性能不佳，可以尝试调整神经网络结构(如增加隐藏层节点数)、优化算法(如调整学习率)或训练参数(如增加训练轮数)等，以提高模型性能。
5.使用训练好的神经网络模型对新的鸢尾花数据进行分类预测。

数据集文件lris_data.txt，我是去网上找的数据集，刚好和视频的一样，有150条数据
https://gairuo.com/file/data/dataset/iris.data

创建pytorch_data_loader.py文件来加载数据
创建`class iris_dataloader(Dataset)`类来加载数据。我们必须要重写下面三个方法，因为我们这里继承了Dataset
**1.初始化方法**：完成对数据集的加载功能，引入assert断言来判断是否数据集存在，不存在就输出后面那句。读入数据，使用字典对每行的最后一列数据进行替换。
**z值化(z-score、标准化)**：将原始数据转换为均值为0、标准差为1的标准分布形式。有利于它将来神经网络训练过程中的一个数值稳定性，提高性能。数据-均值的差除以标准差。
我们要将数据类型转换为张量tensor,所以要先转换为np，再转换。其中from_numpy实现零拷贝内存共享转换，即np数据的变化，张量data的数据也会一同变化。
```python
def __init__(self,data_path):
        self.data_path=data_path
        assert os.path.exists(self.data_path),"dataset does not exits"
        df=pd.read_csv(self.data_path,name=[0,1,2,3,4])
        d={"setosa":0,"versicolor":1,"virginica":2}
        df[4]=df[4].map(d)
        data=df.iloc[:,0:4]
        label=df.iloc[:,4:]
        data=(data-np.mean(data))/np.std(data) #z值化
        self.data=torch.from_numpy(np.array(data,dtype='float32'))
        self.label=torch.from_numpy(np.array(label,dtype='int64'))
        self.data_num=len(label)
        print("当前数据集的大小：",self.data_num)
```
**2.返回数据集大小的函数：** 因为将来我们在训练模型的时候，可能会把数据集拆分成几个批量。然后pytorch需要知道你的数据有多少，才能按照批量大小进行一个封装。
```python
def __len__(self):
        return self.data_num
        ```
**3.__getitem__函数**:获取数据集的样本，参数index是样本的索引，我们需要从数据集中读取。希望通过索引获取数据，所以我们这里要先把数据转换成可迭代对象，如列表。
```python
def __getitem__(self, index):
        self.data=list(self.data)
        self.label=list(self.label)
        return self.data[index],self.label[index]
        ```
        