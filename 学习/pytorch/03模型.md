新建pytorch_nn.py文件
安装conda install tqdm
torch.nn里面封装了很多神经网络常用的功能
torch.optim是一个优化器，可以自动完成梯度下降算法
我们定义的神经网络是三层：输入层、隐藏层、输出层
创建NN类`class NN(nn.Module):`
**1.初始化**：定义网络的层级结构  Linear线性层wx+b
```python
def __init__(self,in_dim,hidden_dim1,hidden_dim2,out_dim):
        super().__init__()
        self.layer1=nn.Linear(in_dim,hidden_dim1)
        self.layer2=nn.Linear(hidden_dim1,hidden_dim2)
        self.layer3=nn.Linear(hidden_dim2,out_dim)
        ```
**2.forwad函数前向传播**：定义网络的功能，当网络接收数据的时候，要做的处理。就是先进入第一层，再进入第二层，再进入第三层，返回最终结果
```python
def forward(self,x):
        x=self.layer1(x)
        x=self.layer2(x)
        x=self.layer3(x)
        return x
        ```
**定义计算环境**(外面) 看是GPU还是CPU
```python
device=torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```
**划分训练集、验证集、测试集(只能做一次)**
```python
custom_dataset=iris_dataloader("lris_data.txt")
train_size=int(len(custom_dataset)*0.7)
val_size=int(len(custom_dataset)*0.2)
test_size=len(custom_dataset)-train_size-val_size
train_dataset,val_dataset,test_dataset=torch.utils.data.random_split(custom_dataset,[train_size,val_size,test_size])
```
他们的划分也可以直接按照比例划分.-1指的是剩下的数据
```python
train_dataset,val_dataset,test_dataset=random_split(custom_dataset,[0.7,0.2,-1])
```
**数据加载，按批量的封装**
`shuffle=True`:前面抽取数据后，对剩余数据进行打散，后面再抽取数据
`len(train_loader)`抽取的轮数，若总共160张图片,batch_size为16，那么就是10.
```python
train_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)
val_loader=DataLoader(val_dataset,batch_size=1,shuffle=False)
test_loader=DataLoader(test_dataset,batch_size=1,shuffle=False)
print("训练集的大小：",len(train_loader)*16,"验证集的大小：",len(train_loader),"测试集的大小：",len(test_loader))
```
**DataLoader**负责将数据集（`Dataset`）封装成可迭代对象(批处理数据生成器)，实现数据的批量获取、混洗、多进程加载等功能。**DataLoader 不是直接将数据加载到模型中，而是为模型训练准备数据的高效管道​**​，放在CPU内存上
**定义推理过程，可以返回准确率**将来可以在验证和测试阶段用上
设置为评估模式，`acc_num`是验证准确的数量，接着使用一个上下文管理器去约束我们的推理过程中，不要改变模型里边的权重参数，做推理嘛实际上是在做测试，跟训练不一样，训练是为了改变模型里面的参数，让模型得到一个更准确的结果
`predict_y=torch.max(outputs,dim=1)[1]` outputs第0维是训练数据的数量，第1维才是结果(`dim=1` 表示在​**​类别维度​**​上查找最大值) `.max()`返回的是一个元组(values,indices)即(概率，类别索引)
对计算正确的数量做一个累加，.eq比较我们当前这个模型的预测结果和当前的label真值是否相等，若相等，就累加。.sum()是因为我们输入的数据是batch_size个的，要统计这里面要多少个数据是正确的。acc_num是对每一批量做一次累加。.item是取到结果的数值
计算准确率：识别正确的数量除以总样本数
```python
def infer(model,dataset,device):
    model.eval()
    acc_num=0
    with torch.no_grad(): #禁用自动求导，减少内存消耗并加速计算，在推理阶段不需要计算梯度
        for data in dataset:
            datas,label=data
            outputs=model(datas.to(device))#前向传播，获取原始输出
            predict_y=torch.max(outputs,dim=1)[1]
            acc_num+=torch.eq(predict_y,label.to(device)).sum().item()
    acc=acc_num/len(dataset)
    return acc
    ```
  outputs是一个概率分布张量，形状为`[batch_size,numm_classes]即[样本数，类别数]`如
  ```
  [[0.1, 0.8, 0.1],  # 样本1的各类别概率
 [0.6, 0.3, 0.1],  # 样本2的各类别概率
 ...]
 ```
  因为我们的设备是在CPU的，所以可以省略to(device)
  **所有PyTorch张量默认创建在CPU**
**定义训练、验证、测试过程main函数**
epoch学习的次数，就是训练多少次
因为数据的特征数量只有4个，所以in_dim为4，还有我们这里只有三个类别，所以在定义model的时候out_dim是为3
.to(device)把定义好的模式输送到设备里面
使用交叉熵损失函数：可以衡量当前模型预测结果跟真值之间的一个距离
pg是模型里面需要梯度的参数

`requires_grad` 是 PyTorch 中每个张量 (Tensor) 的一个属性，用于控制是否需要对该张量进行梯度计算。当设置为：`True` 时，PyTorch 会跟踪所有针对该张量的操作，并构建计算图，以便后续可以通过反向传播计算梯度。

定义优化器optimizer，对可迭代要训练的参数进行更新。`Adam` 是 PyTorch 中实现的一种自适应学习率优化算法

PyTorch 的 API 被组织在 `torch` 模块下，几乎所有的核心功能都通过 `torch.` 前缀访问：主要是为了避免与Python标准库或其他函数名冲突
- 张量操作：`torch.tensor()`, `torch.zeros()`, `torch.ones()`, `torch.cat()` 等
- 数学函数：`torch.sum()`, `torch.mean()`, `torch.sqrt()` 等
- 随机数生成：`torch.rand()`, `torch.randn()`, `torch.randint()` 等
- 设备管理：`torch.device()`, `torch.cuda.is_available()` 等

os.getcwd()获取当前文件的绝对地址
.zeros(1)是生成一个标量，就是一维的数值0
train_bar是训练的进度条。`tqdm` 是一个 Python 库，用于在循环中创建进度条，使代码的执行过程可视化。**file**指定输出位置为标准输出(终端)。`ncols=100`：设置进度条的宽度为 100 个字符
label=label.squeeze(-1)把标签的最后一个维度去掉。如`[[3],[2],[1]]变为[3,2,1]`
`sample_num+=data.shape[0]` 返回当前数据的形状，第0维是当前数据批量大小。表示当前它处理过的批量的数量
对优化器进行清理，对过去的梯度清零
.max返回的是一个二元组，第一个是值，第二个是索引，我们要的是索引
loss是对模型输出 `outputs` 应用 softmax 函数，将原始分数转换为概率分布，1. 计算预测概率分布与真实标签之间的负对数似然损失
```python
def main(lr=0.005,epochs=20):
    model=NN(4,12,6,3).to(device)
    loss_f=nn.CrossEntropyLoss() #初始化实例对象
    pg=[p for p in model.parameters() if p.requires_grad]
    optimizer=optim.Adam(pg,lr=lr)
    #权重文件存储路径
    save_path=os.path.join(os.getcwd(),"results/weights")
    if os.path.exists(save_path) is False:
        os.makedirs(save_path)
    #开始训练
    for epoch in range(epochs):
        model.train()
        acc_num=torch.zeros(1).to(device)
        sample_num=0
        train_bar=tqdm(train_loader,file=sys.stdout,ncols=100)
        for datas in train_bar:
            data,label=datas
            label=label.squeeze(-1)
            sample_num+=data.shape[0]
            optimizer.zero_grad()
            outputs=model(data.to(device))
            pred_class=torch.max(outputs,dim=1)[1]#torch.max返回值是一个元组，第一个元素是max值，第二个是max索引
            acc_num=torch.eq(pred_class,label.to(device)).sum()
            loss=loss_f(outputs,label.to(device))
            loss.backward() #求导
            optimizer.step()#更新参数
            train_acc=acc_num/sample_num
            train_bar.desc="train epoch[{}/{}] loss:{:.3f}".format(epoch+1,epochs,loss)
        val_acc=infer(model,val_loader,device)
        print("train epoch[{}/{}] loss:{:.3f} train_acc{:.3f} val_acc{:.3f}".format(epoch+1,epochs,loss,train_acc,val_acc))
        torch.save(model.state_dict(),os.path.join(save_path,"nn.pth"))
        #每次数据集迭代之后，要对初始化的指标清零
        train_acc=0
        val_acc=0
    print("Finshed Training")
    test_acc=infer(model,test_loader,device)
    print("test_acc:",test_acc)
if __name__=="__main__":
    main()
 ```
  运行结果
    ![[Pasted image 20250607160646.png]]![[Pasted image 20250607160733.png]]