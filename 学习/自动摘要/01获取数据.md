新建newscrawling.py文件
获取想要的网站链接，来爬取数据。使用xpath技术保存到./data/网易新闻
```python
import os
import requests
from lxml import etree
def spider():
    # 输入多个网易新闻的链接
    urls = [
        'https://www.163.com/news/article/J0PLPNPN000189PS.html',
        'https://www.163.com/dy/article/J0PL13LS0514R9P4.html',
        'https://www.163.com/news/article/J0PBVD1P0001899O.html',
        'https://www.163.com/dy/article/J0NFUAAA0514R9KQ.html',
        'https://www.163.com/dy/article/J0O8R59J0512B07B.html',
        'https://www.163.com/dy/article/J0O014FQ0534A4SC.html',
        'https://www.163.com/dy/article/J0MU1JBS05129QAF.html',
        'https://www.163.com/dy/article/J0N2S3QS0514R9OJ.html',
        'https://www.163.com/dy/article/J0LL8RUV0512B07B.html',
        'https://www.163.com/dy/article/J0MFLTE50530JPVV.html',
        'https://www.163.com/dy/article/J0MM2OJ70530JPVV.html',
        'https://www.163.com/dy/article/J0LGE28G051492T3.html',
        'https://www.163.com/dy/article/J0MKL7SP05198CJN.html',
        'https://www.163.com/dy/article/J0LKD7JB051492T3.html',
        'https://www.163.com/news/article/J0L7M0FD00019K82.html',
        'https://www.163.com/dy/article/J0L6DVFV0550A0OW.html',
        'https://www.163.com/dy/article/J0L5JTJM053469M5.html',
        'https://www.163.com/dy/article/J0KVM4N7051482MP.html',
        'https://www.163.com/dy/article/J0KRGULN0550A0OW.html',
        'https://www.163.com/dy/article/J0KGFG76053469LG.html',
        'https://www.163.com/dy/article/J0KGERPB0514D3UH.html',
        'https://www.163.com/news/article/J0KFPCM90001899O.html',
        'https://www.163.com/dy/article/J0KE35A3053469LG.html',
        'https://www.163.com/dy/article/J0KDDDJK05198CJN.html',
        'https://www.163.com/dy/article/J0KBDTMP0514R9P4.html',
        'https://www.163.com/dy/article/J0KACCAV05129QAF.html',
        'https://www.163.com/dy/article/J0K17UNJ0514R9OJ.html',
        'https://www.163.com/news/article/J0JVDDDS0001899O.html',
        'https://www.163.com/dy/article/J0J406EC0512B07B.html',
        'https://www.163.com/news/article/J0HUHEC30001899O.html',
        'https://www.163.com/dy/article/J0HT890L051492T3.html',
        'https://www.163.com/dy/article/J0GM4SKH0514TTN3.html',
        'https://www.163.com/news/article/J0HSIDJH0001899O.html',
        'https://www.163.com/news/article/J0HL8LD10001899O.html',
        'https://www.163.com/news/article/J0HHRAFD0001899O.html',
        'https://www.163.com/news/article/J0HECC360001899O.html',
        'https://www.163.com/dy/article/J0FQSN0J0514R9P4.html',
        'https://www.163.com/dy/article/J0HAGIFM0530WJIN.html',
        'https://www.163.com/news/article/J0FRLHG90001899O.html',
        'https://www.163.com/dy/article/J0G90GS7051492T3.html',
        'https://www.163.com/dy/article/J0G9QOOQ051482MP.html',
        'https://www.163.com/dy/article/J0FVCNDF051482MP.html',
        'https://www.163.com/dy/article/J0FPHF6B051482MP.html',
        'https://www.163.com/news/article/J0FQ8V5F0001899O.html',
        'https://www.163.com/news/article/J0FHDQK40001899O.html',
        'https://www.163.com/news/article/J0FA5DIN0001899O.html',
        'https://www.163.com/news/article/J0F72AGJ0001899O.html',
        'https://www.163.com/dy/article/J0F0EAPV05129QAF.html',
        'https://www.163.com/dy/article/J0EVEVA105346RC6.html',
        'https://www.163.com/dy/article/J0EK53L60514R9OJ.html'
    ]
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0'
    }
    save_path = './data/网易新闻'
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    for index, url in enumerate(urls, start=1):
        filename = os.path.join(save_path, f'news_{index}.txt')
        with open(filename, 'w', encoding='utf-8') as fp:
            print(f'正在爬取新闻 {index}')
            response = requests.get(url, headers=headers)
            page_text = response.text
            tree = etree.HTML(page_text)
            p_list = tree.xpath('//*[@id="content"]/div[2]//p')
            for p in p_list:
                text = p.xpath('./text()')
                if text:
                    text = text[0]
                    fp.write(text)
                    fp.write('\n')
        print(f'新闻 {index} 爬取结束！')
if __name__ == "__main__":
    spider()
    ```
