无论是大语言模型还是多模态模型，训练过程都是先进行预训练，然后进行SFT有监督微调，最后进行强化学习。后面两也统称为后训练。

预训练的目标是让大模型学会补充，如在一个不完整的句子后面填充下一个字。循环补全成一个句子。transformer里面的解码器就是专门去做这种的。编码器最后脱离出来变成BERT，做的是完形填空，填充中间的字。 学习通用知识

SFT有监督微调是让模型看懂我们的问题(听我们的指令)。输送到大模型的是instruction和question拼接起来的字符串。在大模型眼里，预训练和这个微调都是预测下一个token。我们给模型一个案例就是one shot.

强化学习让模型超越我们人类的行为。在测试集和训练集分布相同的情况下（如都是数学题），SFT的效果更好，不相同那就是强化学习更好（如训练集是数学，测试是物理）。
目的是提升模型的泛化性，泛化性就是处理分布不同的能力；可能提升模型性能。
一般强化学习在大模型上才有效果。
涌现能力指你的技术投入了很多的计算资源，扩大了数据量，但是在一个小的模型上，它的性能提升不明显。在大模型上就提升得很明显。


![[Pasted image 20250703170601.png]]
RLHF这个是很常用的。第一步是做SFT，第二步训练一个奖励模型（老师。给定问题和回答，老师判断真假或打分），准备多个问题，一个问题让模型生成多个答案。这样就产生了数据集，然后让人或AI对这些答案打分。(这个AI可以和生成答案的AI不同).然后我们产生了新的数据集，包括问题、答案、打分。拿这个数据集来训练老师
第三步给定一个问题给SFT训练后的AI，它会生成多个答案，然后让这个老师去判断这个回答是否正确。若不正确，就会产生loss，然后去更新模型的参数。

GRPO是直接面向结果的，我们看不懂它们的过程。
使用GPU训练会更好，CPU它要干的活多，GPU只是计算。

**微调**就是在提高大模型某部分的能力，再次进行sft,广义上还可以再进行强化学习。
**蒸馏**
![[Pasted image 20250703181822.png]]
从0-1可以去做微调，微调的目的就是让模型在没有见过的领域学习新的知识，适用于参数量较大的模型
从1-N可以做蒸馏，蒸馏一定是学生模型的参数比教师模型的参数要小。蒸馏的目的是为了得到一个大模型的mini版本，节省了重新训练小模型的时间。



|     | 预训练    | 微调       |
| --- | ------ | -------- |
| 目的  | 预测下一个词 | 完成特定任务   |
| 样本  | 文字片段   | 完整问答     |
| 方法  | 无监督    | 有监督(有标签) |
| 误差  | 颗粒度是片段 |  是完整回答   |
|     |        |          |
微调的误差要是使用交叉熵计算，就是计算答案内每个单词的误差的平均值，再计算多个答案的平均误差得到批次误差，用它来修正微调的模型。(聚焦回答问题的能力)

##### 评价方式，评估模型
**客观问题** 即可以选ABCD的那种，可以通过正确率
**主观问题** 通用能力 如写特性诗。可以使用能力较强公正的大模型来打分
**专属**  人类打分

**进行微调的方式**
一：调用在线API接口。属于在线服务，整个参数权重不能下载
二：微调开源大模型。