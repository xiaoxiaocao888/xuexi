##### LoRA微调模型部署模式
**1.独立LoRA适配器模式**
特点：仅保存和部署低秩矩阵A和B，推理时动态加载LoRA权重，并与底座模型相结合
优势：模型存储高效，切换灵活性高
**2.合并权重模式**
特点：将LoRA适配器与底座模型权重合并，形成单一模型文件
优势：推理无额外开销
**3.动态加载模式**
特点：通过统一内存管理，动态加载多个LoRA适配器，支持高并发多任务推理
优势：支持数千个适配器同时服务(可能每个适配器适用于不同的业务领域，但都依赖于同一个底座模型)
**4.渐进式压缩模式**
特点：训练中逐渐减低底座模型权重，最终仅保留LoRA适配器，实现模型压缩
优势：极致压缩，参数减少90%以上
**5.量化部署模式**
特点：将合并后的模型量化为GGUF，进一步减少模型体积
优势：显存占用极低

**模型权重合并的算法基础**：合并之前有三个矩阵，一个原始矩阵w0，一个B矩阵，一个A矩阵，当样本数据进来的时候，是wo和样本数据x相乘，然后B矩阵和A矩阵先做一个点接，再相乘，然后将什么两个相乘的结果相加。模型合并算法是这样的(w0+B·A)x 
算法的实现：在train_lora_peft.py文件的最后面for循环里面加上
```
model.merge_and_unload()      model.save_pretrained(os.path.join(args.save_dir,"lora",args.lora_name))
```
merge_and_unload方法就是把LoRA模型的权重追加到底座模型，然后再移除LoRA权重
最后做一个保存。
运行该文件

MiniMindLM是底座模型最外层的一个类