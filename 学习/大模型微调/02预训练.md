1.克隆项目到本地
```
git clone https://github.com/cn-farmer/minimind.git
```
2.准备环境 要下载的包都放在requirements.txt文件里面了.-y的意思是询问确认时自动确认yes
```
conda create --name xunlian
conda activate xunlian
pip install python=3.10 -y #要是不行就是conda install
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```
3.下载数据集pretrain_hq.jsonl 到dataset目录下
把该文件修改为pretrain_hq_all.jsonl,然后复制里面的一部分数据命名为pretrain_hq.jsonl。
```
https://www.modelscope.cn/datasets/gongjy/minimind_dataset/file/view/master/pretrain_hq.jsonl?id=68909&status=2
```
4.进行预训练，即执行train_pretrain.py文件
在主函数里面
 `parser.add_argument("--num_workers", type=int, default=1)`
 这些是开始的默认参数或者是我们手动输入参数的一个处理
 `lm_config = LMConfig(dim=args.dim, n_layers=args.n_layers, max_seq_len=args.max_seq_len, use_moe=args.use_moe)`
 这个是去加载它的一个参数定义的信息。
 做完预训练后应该会有应该输出的模型，对于pytorch来说是一个PTH文件
 如果没有看到PTH文件的生成，我们使用ctrl+f输入pth
 可以看到该文件的生成和.save_interval有关
 ![[Pasted image 20250603210725.png]]
 进而查找，发现和这个参数有关，我们试着把这个间隔缩小，把100改为5，重新执行该文件进行预训练
![[Pasted image 20250603210849.png]]
我们在huggingface上面看到的模型一般都是gguf的文件，那是作为部署用的。
![[Pasted image 20250603214452.png]]
我们可以去评估一下该模型，进入eval_model.py文件，把default改为0
![[Pasted image 20250603215232.png]]
可以跑通
![[Pasted image 20250603215551.png]]

简单过一下train_pretrain.py文件里面main函数的参数
`epochs`: 轮次，默认为1
`batch_size`:批次，默认32
`learning_rate`:学习率
`save_interval`:保存的周期，多少个批次做一个保持，默认5
`dim`：向量的维度，默认是512
`n_layers`：模型有多少层，默认为8
`max_seq_len`:最大的序列长度 和语料有关，一般不修改
`data_path`：语料的路径
`torch.manual_seed`：创建随机种子，固定下来可以重复去复现

X、Y的数据都是32行的，因为batch_size批次是32
#### 修改参数设置，让预训练速度更快
`epochs`轮次为1，`batch_size`  批次为4，4组数据为一批
`log_interval`:日志批次该为2，两个批次输出一次日志(默认为100)
`dim`：向量改为128     `n_layers`:层次改为2层(模型参数就小很多)
执行train_pretrain.py文件。
![[Pasted image 20250604171307.png]]
![[Pasted image 20250604171406.png]]
##### 我们还可以去修改词表
进入这个配置里面`lm_config = LMConfig` 在这个LMConfig类里面把`vocab_size` 改为640，就是要重新训练一下这个分词
来到train_tokenizer.py文件的train_tokenizer()函数里面，把这个也修改为640
`trainer = trainers.BpeTrainer(vocab_size=640,)`
然后去训练 运行train_tokenizer.py文件，把这个词表按照640的大小去重做训练，会把数据保存在model\minimind_tokenizer目录下面
然后再去跑train_pretrain.py文件，可以看到参数量少了很多。运行完可以看到模型是pretrain_128.pth，然后就去看评估
对eval_model.py文件的main函数的`dim` 改为128，`n_layers`改为2，然后运行。
输入1   鉴别。可以看到有很多乱码的地方，因为vovab_size设置得很小。