transformer本质上是一个Encoder-Decoder架构。因而中间部分的transformer可以分为编码组件和解码组件。
编码组件由多层编码器(Encoder)组成(论文是6层，我们实际使用可以使用其他层数)，解码组件由相同层数的解码器(Decoder)组成。
每个编码器由两个子层组成:Self-Attention层(自注意力层)和Position-wise Feed Forward Network(前馈网络FNN)。每个编码器的结构都是相同的，但是它们使用不同的权重参数。
编码器的输入会先流入Self-Attention层。它可以让编码器在对特定词进行编码时使用输入句子中的其他词的信息(如在翻译词语时，不仅关注当前词，还会关注其他词的信息，从而更好地编码这个词，如可以分清楚英文里面的it指代的是哪个)。然后Self-Attention层的输出流入前馈网络。
解码器是在中间多了个注意力层(Encoder-Decoder Attention)，是用来帮解码器关注输入句子的相关部分(类似于seq2seq模型中的注意力)
`seq2seq模型主要目的是处理输入和输出都是长度可变的序列任务,就是学习如何将一个序列(如一个句子、一段语音、一个时间序列)转换为另一个序列`

和通常的NLP任务一样。首先，我们使用词嵌入算法(Embedding)将每个词转换为一个词向量,维度为512。嵌入仅发生在最底层的编码器中。所有编码器都会接收到一个大小为512的向量列表--底部编码器接收的是词嵌入向量，其他编码器接收的是上一个编码器的输出。这个列表大小是我们可以设置的超参数--基本上这个参数就是训练数据集中最长句子的长度。
对输入序列完成嵌入操作后，每个词都会流经编码器的两层

##### Self-Attention机制
结构图如下。
![[Pasted image 20250612212221.png]]
Q(Query),K(Key)和V(Value)三个矩阵均来自同一输入。计算步骤：先计算Q和K的点积，为防止结果过大，然后利用Softmax操作将其结果归一化为概率分布，再乘以矩阵V就得到权重求和的表示。![[Pasted image 20250612212543.png]]
###### 例子
**一：** 对编码器的每个输入向量(在本例子中，即每个词的词向量)创建三个向量：Q、K、V。它们是通过词向量分别和3个矩阵相乘得到的，这3个矩阵通过训练获得。
注意哦，这些向量的维数小于词向量的维数，新向量的维数是64，而embedding和编码器输入/输出向量的维数为512.新向量不一定非要更小，这是为了使多头注意力计算保持一致的结构性选择。
**二：** 计算注意力分数。假设我们正在计算第一个词"Thinking"的自注意力。然后根据"Thinking"这个词，对句子中的每个词都计算一个分数。分数对应在编码"Thinking"时，对其他词放置的注意力。这些分数，是通过计算"Thinking"的Query向量和需要评分的词的Key向量的点积得到的。如果我们计算句子中第一个位置词的注意力分数，则第一个分数是q1和k1的点积，第二个分数是q1和k2的点积。
**三：** 将每个分数除以根号dk(dk是Key向量的维度)。目的是在反向传播时，求梯度更加稳定。实际上，也可以除以其他数
**四：** 将这些分数进行Softmax操作。Softmax将分数进行归一化处理，使得它们都为正数并且和为1.
**五：** 将每个Softmax分数分别与每个Value向量相乘。分数高，相乘值越大，放的注意力越多。
**六：** 将加权Value向量(上一步求得的向量)求和。这样就得到了自注意力层在这个位置的输出。
###### 实际实现是以矩阵形式进行
**一：** 计算Query、Key、Value矩阵。首先，将所有词向量放到一个矩阵X中，然后分别和3个我们训练过的权重矩阵相乘，得到Q、K、V矩阵
矩阵X中的每一行，表示输入句子中的每一个词的词向量(长度为512)
矩阵Q、K、V中的每一行，分别表示Q向量、K向量、V向量(长度为64)
![[Pasted image 20250613094744.png]]
**二：** 计算自注意力。
![[Pasted image 20250613095106.png]]

##### 多头注意力机制(Multi-head Attention)
首先，通过8个不同的线性变换对Query、Key和Value进行映射，然后，将不同的Attention拼接起来；最后，再进行一次线性变换。
每一组注意力用于将输入映射到不同的子表示空间，这使得模型可以再不同子表示空间中关注不同的位置。整个计算过程可表示为：
![[Pasted image 20250613100235.png]]
在多头注意力下，我们为每组注意力单独维护不同的Query、Key和Value权重矩阵，从而得到不同的Q、K、V矩阵。如上面所述。
按照上面的方法，使用不同的权重矩阵进行8次自注意力计算，就可以得到8个不同的Z\boldsymbol{Z}Z矩阵。
因为前馈神经网络层接收的是1个矩阵(每个词的词向量),而不是上面的8个矩阵。接下来要：
- 把8个矩阵{Z0,,Z1,...,Z7}拼接起来
- 把拼接后的矩阵和一个权重矩阵W0相乘
- 得到最终的矩阵Z，这个矩阵包含了所有注意力头的结果，这个矩阵会输入到FFN层
![[Pasted image 20250613101053.png]]
Multi-head Attention的本质是，在参数总量不变的情况下，将同样的Query,Key,Value映射到原来的高维空间的不同子空间中进行Attention的计算，在最后一步再合并不同子空间中的Attention信息。这样降低了计算每个head的Attention时每个向量的维度。在某种意义上防止了过拟合；由于Attention在不同子空间中有不同的分布，Multi-head Attention实际上寻找了序列之间不同角度的关联关系，并在最后拼接这一步骤中，将不同子空间中捕获到的关联关系再综合起来。

##### 位置前馈网络(Position-wise Fedd-Forward Networks)
位置前馈网络就是一个全连接前馈网络，每个位置的词都单独经过这个完全相同的前馈神经网络。其由两个线性变换组成，即两个全连接层组成，第一个全连接层的激活函数为ReLU激活函数。可表示为
![[Pasted image 20250613102756.png]]


###### 残差连接和层归一化
每个编码器的每个子层(Self-Attention和FNN层)都有一个残差连接，再执行一个层标准化操作，整个计算过程可表示为：
sub_layer_output=LayerNorm(x+SubLayer(x))
![[Pasted image 20250613103250.png]]

###### 掩码
在transformer模型里面涉及两种掩码，Padding Mask和Sequence Mask。其中，Padding Mask在所有的scaled dot-product attention里面要用，sequence Mask只有在Self-Attention里面用到。
**Padding Mask** 的目标是在自注意力计算中，阻止模型关注(赋予权重)任何(`<PAD>`)标记的位置，解决批次内序列长度不一致导致的填充问题。它是一个二进制张量(Binary Tensor)或布尔张量(Boolean Tensor).形状通常是(batch_size,sequence_length)或为了适配多头注意力广播成(batch_size,1,1,sequence_length) 。值0或False表示该位置是真实单词/标记。值为1或True表示该位置是填充标记(`<PAD>`).
详细步骤如下
- 计算原始注意力分数 (S):模型像往常一样计算 Query 向量 和 Key 向量 之间的相似度（通常是点积 Q * K^T），得到一个原始注意力分数矩阵 S。S的形状是 (batch_size, num_heads, query_sequence_length, key_sequence_length)。`S[i, j]`表示第 i个查询位置（对应序列中的第 i 个词）对第 j个键位置（对应序列中的第 j个词）的原始关注度分数。
- 应用 Padding Mask (M):
  根据 Padding Mask 张量（标识了哪些 Key 位置是 `<PAD>`），创建一个 掩码矩阵 M。这个矩阵 M的形状需要和注意力分数矩阵S完全一致
    规则：对于序列中的每一个 Key 位置 j（也就是序列中第 j个词）：
    如果位置 j是 `<PAD> `( 即1/ True)，那么在掩码矩阵 M 的整个第 j 列上，都设置为一个非常大的负数（比如 -1e9）或负无穷 (-inf)。相反都设置为 0。    
    为什么是列？因为 Key 的位置 j对应着矩阵的列。屏蔽掉第 j列，意味着所有 Query 位置（无论 `i` 是真实单词还是 `<PAD>`）在计算对第 j个 Key 的注意力权重时，该权重都会被压制。
- 计算掩码后的注意力分数 (`S_masked`): 将原始注意力分数 S 与掩码矩阵 M相加：  S_masked = S + M
 - **应用 Softmax:** 对 `S_masked` 的**每一行**（即**每个 Query 位置 `i` 对所有 Key 位置 `j` 的分数向量**）应用 Softmax 函数，计算最终的注意力权重分布 `A`：  
    `A = softmax(S_masked, dim=-1)` (在最后一个维度 Key 维度上做 Softmax)
    - **Softmax 的特性：** 它会将输入向量转换为一个概率分布（和为1)正常的会根据与其他分数的相对大小获得一个**合理的、非零的权重**。 对于填充 Key 位置 j结果是一个极其接近于 0 的值    
- **计算加权和:** 最后，使用注意力权重 `A` 对 Value 向量 (`V`) 进行加权求和，得到每个 Query 位置的新表示 `Output`：  
    `Output_i = sum_j(A[i, j] * V_j)`
    - **关键结果：** 因为对于 `<PAD>` 位置 `j`，其权重 `A[i, j] ≈ 0`，所以无论对应的 `V_j` 是什么（通常是零向量或特殊嵌入），它对最终 `Output_i` 的贡献都**微乎其微，几乎被完全忽略**。

**为什么是 Key 位置？**
- 自注意力机制中，一个 Query 位置（想知道“我该关注谁”）会去查询（Query）所有 Key 位置（代表“我是谁，我能提供什么信息”）。
- 屏蔽 Key 位置 `j` 意味着：“无论你是哪个 Query 位置（`i`），你都**不应该**从这个位置 `j`（因为是 `<PAD>`）获取任何有意义的信息”。所以，在计算 Query `i` 的注意力分布时，位置 `j` 的分数被设为 `-inf`，使其权重为 0。
- 这也自然地屏蔽了 `<PAD>` 位置本身作为 Query 时的输出计算，因为当 `i` 是 `<PAD>` 位置时，它对所有 Key `j` 的注意力权重中，真实 Key 位置可能有正常权重，但填充 Key 位置的权重会被强制为 0。虽然 `<PAD>` 位置的输出通常不会被使用，但计算过程本身也被约束了。

Sequence Mask是为了使得Decoder不能看见未来的信息。具体的做法：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每个序列上，就可以达到我们的目的。

对于Decoder的Self-Attention，里面使用到的scaleddot-productattention，同时需要PaddingMask和SequenceMask，具体实现就是两个Mask相加。其他情况下，只需要PaddingMask。