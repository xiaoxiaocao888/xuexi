low rank adaptation里面的rank就是秩的意思。低秩适配
矩阵中的秩是其行向量或列向量的极大线性无关组中向量的个数。通俗来说，秩衡量了矩阵中包含的真正独立信息的量，是理解线性变换结构的关键指标

图的左边是一个预训练的权重，右边是一个A，B矩阵相乘，相乘得到的形状和预训练矩阵的形状是一样。在微调的时候，对这个A矩阵以正态分布的形式来做一个初始化，B矩阵全部初始化为0，通过微调来对A和B的权重进行优化，就可以达到不修改原始预训练权重的情况下，通过对A B矩阵的训练去接近全参微调的效果。A B矩阵的参数量是和它的秩有关系的，如果把秩订得很小，那参数量可能只有5%那样。
如A矩阵是m×r，B矩阵是r×n。r是一个重要的超参数，如果在A把r加一列，在B把r加一行。那么AxB的结果是不受影响的。我们可以根据微调的效果来进行调整，如果不理想，可以加大r，那么参数量就增大。
![[Pasted image 20250604223248.png]]

LoRA 算法的核心概念就是对矩阵进行压缩。
在工程实践上来说，在整个transformer架构中，对其中的哪个矩阵进行训练效果会比较好？矩阵的秩到底是设置成2、4还是6，AB矩阵是全部初始化为0还是1，学习率又应该怎么去设置，Alpha缩放系数怎么考虑，怎么避免训练的不稳定性，怎么去避免灾难性遗忘，微调之后的模型是在所有领域表现都会更好还是只是对某一些领域表现会更好，这些都是LoRA 算法在落地的时候需要一个一个去验证尝试的

这个是Word2Vec词向量算法中相对立的算法。
一种是通过相邻的词来预测它的中心词，也叫连续磁带模型CBOW
另外一个是通过中心词来预测相邻的词汇，即skip-gram
![[Pasted image 20250604225022.png]]
FFN层是先把矩阵放大再缩小，在该过程中通过激活函数的处理可以让模型学习到更多有用的特征,
![[Pasted image 20250604225249.png]]