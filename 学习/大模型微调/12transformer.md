如要对”我爱你“进行一个翻译。首先是要以数字的形式进入模型。我们会有一个模型词汇表。如“我”对应的索引可能是102，“你”对应的索引可能是205，“爱”对应的索引可能是309。所以输入 可能是`[102,309,205]`
接着我们的数字队列就会进入到模型的嵌入层。(因为数字队列并不能传递出它们在句子中丰富的含义)每一个数字会转换成512维的向量，所以输出是3×512的矩阵
接着加入位置编码。每一个位置的编码包含与词嵌入相同数量的特征，而这个矩阵捕获了每个词在句子中相对或绝对的位置。也是一个3×512向量
![[Pasted image 20250606100000.png]]
接着将位置编码向量直接加在输入嵌入向量上，就是进行简单的相加操作，得到新的3×512矩阵。它既包含了词的语义信息也包含了它的位置信息。
接着进入核心 多头注意力机制。将矩阵从n×512变换成8个n×64的矩阵。
进行这种转换的原因：
- 信息分解：将512维的向量分解成8个64维的向量允许模型将注意力集中在输入的不同"方面"上。每个头可以学习到序列不同部分的不同表示。这种多角度的理解可以帮助模型更准确地捕获语义信息。
- 并行处理：多个头可以并行工作，这样可以同时处理多种信息流，增加处理的速度和效率
- 模型复杂度：通过增加处理头的数量，我们可以在不显著增加单个注意力头复杂性的情况下，增加整个模型的复杂性和表达能力。每个头关注不同的信息，组合起来就能形成一个全面的信息视角。
- 捕获长距离依赖：在自然语言中，词与词之间可能存在跨越长距离的关系。多头注意力通过分别关注句子的不同部分，帮助模型更好地捕捉这些远距离的依赖。
![[Pasted image 20250606100849.png]]
接着就是矩阵相乘，输出变为3×3
![[Pasted image 20250606101845.png]]
接着就是进入缩放层。可以理解为进行统一的归一化。控制内积(dot products)的大小。这个dk是可以自己设置的，主要是为了后面的softmax函数的梯度不会特别小，避免出现梯度消失。
![[Pasted image 20250606102017.png]]
接着就是mask掩码机制。只有在解码器这个部分使用到。是为了确保模型在生成文字的时候符合自然生成的过程。模型在生成的时候不应该知道后面的词是什么，为了达到这样的一种逻辑的功能，就是将后面的数字设置为一个非常大的负数
![[Pasted image 20250606102341.png]]
接着进行softmax。将一个含任意实数的向量转换成一个实数，在0和1之间的概率分布。将注意力得分转换成概率。进而表明了在给定当前词的情景下，句子中其他词的相对重要性。如对于“我”，由于其后的词被掩码了，只能看到自己，因为“我”对自己的注意力概率就为1.而对于后面的“爱”“你”能看到上文的信息，他就能分配到更多的注意力在相关的词身上。
![[Pasted image 20250606102716.png]]
接着再进行矩阵乘法。就是softmax输出的3×3与3×64矩阵相乘，生成3×64的矩阵。这个输出不仅包含了自己的信息也包含了句子中其它词对它的注意力。通过这个过程，让transformer能够为上下游的任务提供富含上下文信息的一个词表示，从而大大提高了模型处理复杂自然语言任务的能力
![[Pasted image 20250606103959.png]]
总结：我们的输入是一个3×512的矩阵，经过线性转换变成了8个独立的3×64的矩阵，接着进行多个独立的scared dot-product attention操作，输出8个3×64的矩阵。接着进行拼接操作。所有头的注意力完成后，将所有头拼接起来形成一个更大的矩阵3×512。然后再通过一个线性层，是对于不同注意力头的信息进行融合，输出依然是3×512的矩阵
![[Pasted image 20250606104139.png]]
完成多头注意力机制后就会进入**Add & Norm**，就是残差连接和层归一化。残差连接就是将多头注意力机制的输出与它的输入矩阵进行相加。这种直接相加的操作有助于解决深层网络中梯度消失的问题。是的深层网络的训练变得更加稳定和高效。
**层归一化**会对每一个序列在特征维度上进行归一化处理，确保数据在流经网络的各个层时保持稳定。这一步骤是通过计算输入的均值和方差，然后使用这些统计量来归一化输入，使得输出的均值接近0，方差接近1.这样做可以加快训练过程，并且提高模型的泛化能力。
输出依然是一个3×512的矩阵。
接着进入全连接层。
![[Pasted image 20250606105625.png]]
总结。右边的图形那里它是向右移一步，就变成4×512
![[Pasted image 20250606110414.png]]
