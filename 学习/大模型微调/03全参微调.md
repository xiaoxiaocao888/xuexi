运行全参微调程序
一：运行之前，把本地工程做一个同步更新
二：获取微调之前的这个基座模型和这个微调的数据集
三：开启微调，验证微调成果
四：拓展，配一个线上的模型训练监控的功能

先要去训练去一个基座模型，即在train_pretrain.py文件里面，
把`batch_size` 2         `save_interval` 1  
执行该文件
![[Pasted image 20250604193208.png]]
去到下载数据集的网站，
https://www.modelscope.cn/datasets/gongjy/minimind_dataset/resolve/master/sft_mini_512.jsonl
下载sft_mini_512.jsonl文件，到dataset目录下
在服务器上下载的步骤
```python
pip install modelscope
modelscope download --dataset gongjy/minimind_dataset sft_mini_512.jsonl --local_dir ./dataset/
```
然后进行微调，进入train_full_sft.py文件。来到main函数的参数设置那里
把`save_interval` 改为1，`log_interval`改为1，`batch_size`改为2
执行该文件。
![[Pasted image 20250604194119.png]]
然后做模型评估，运行eval_model.py文件，注意里面参数那里model_mode那里要是1.
然后执行。输入1  您好。可以跑通。
![[Pasted image 20250604194321.png]]
我们还可以在魔塔社区里面，点击模型库，搜索minimind，点击后缀为PyTorch的，然后进入里面的模型文件，找到pretrain_512.pth文件。下载到我们的out目录下。它会覆盖之前生成的pretrain_512.pth文件。然后我们再运行一下train_full_sft.py文件微调，再运行eval_model.py文件进行评估。
输入1  你好。

在train_full_stf.py文件main函数的参数里面`use_wandb` 读法是weight and Bias，权重和偏置的意思。如果我们要使用在线的监控工具，把default=True加入；也可以去官网做一个注册 wandb.ai/authorize 复制key。来到项目终端页面输入wandb login 
然后粘贴刚刚的key。然后去添加配置，我这里用的是vscode，弄起来麻烦。