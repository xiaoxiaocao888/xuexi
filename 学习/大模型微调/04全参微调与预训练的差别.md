本节要点：全参微调和预训练的差异，再单步执行全参微调，加深对Transformer架构的一个理解。
先打开train_full_sft.py文件，然后鼠标放到train_pretrain.py文件出右键选择open to the side
就可以同时打开两个文件了
1.预训练处是有梯度累积的一个考虑，在全参微调里面梯度累计默认是1
![[Pasted image 20250604200518.png]]
2.checkpoint名称的保存不一样。
![[Pasted image 20250604200929.png]]
3.模型初始化这里：预训练是直接根据config的配置信息对模型进行初始化
全参微调那里的分词器是一样的，模型也是根据配置信息进行初始化。但是初始化后又多做了一个加工。把权重的信息通过字典的形式load_state_dict进来。做全参微调的时候，会先去加载基座模型。
![[Pasted image 20250604201158.png]]
4.`accumulation_steps`参数，全参默认为1，预训练默认为8
![[Pasted image 20250604201826.png]]
5.`data_path`参数加载的语料也不一样
![[Pasted image 20250604202109.png]]
6.对数据集的包装不一样，使用不同的方法。预训练使用 PretrainDataset，全参用SFTDataset
![[Pasted image 20250604202344.png]]

##### 对比数据集的处理方法
**先看数据集格式：**
sft_mini_512.jsonl是一个conversations对话，里面有多轮对话。人类提问，大模型回答，反复这样。
![[Pasted image 20250604202828.png]]
预训练里面pretrain_hq.jsonl是一个text节点，后面是一段长文本。
![[Pasted image 20250604203042.png]]
transformer属于一种自回归的模型，它在自监督模型里面是通过第一个词预测第二个，然后第一个和第二个来预测第三个，以此类推以这种方式去做预测。
而在微调的时候，数据集格式就不一样，里面是对话的形式，模型在做损失计算的时候，只需要计算模型生成的内容，而人类的提问部分是不会去计算损失的。
**看PretrainDataset和SFTDataset类的区别**：SFTDataset比Pretrain的特别部分：
- 初始化函数里面多了下面两个参数。
begin of sentence是加了一个assistant,相当于把这两个`<s>assistant`连起来做为一个句子的起点，然后从后面开始去做预测，如去计算损失
end of sentence是`</s>`单独把它列出来，
```
self.bos_id = tokenizer('<s>assistant',add_special_tokens=False).input_ids
self.eos_id = tokenizer('</s>', add_special_tokens=False).input_ids
 ```
 - 因为我们的数据集是对话形式的，然后创建了` _create_chat_prompt`这个函数，在下方的`__getitem__`函数里面有用到这个函数。
 - 多了一个生成损失的`_generate_loss_mask`函数。入参input_ids是已经经过分词的数据，在这里设置一个断点，可以看到里面的数据，后面会有很多个0，是因为长度不够使用padding补全补齐的。列数是512，是因为我们设置的max_seq_len是512
   `loss_mask = [0] * len(input_ids)`是设置成了一个1X512的零向量。
 ![[Pasted image 20250604213227.png]]
 在进入循环之前那，对于这个prompt数据是对原始的样本数据进行处理，加了一个system角色的一个指令。循环之后的loss_mask数据里面的1部分的数据代表了要去计算损失，
 因为值为0那些数据×任何数据都还是0，就可以忽略掉。
 1的数据是大模型生成的部分，对应到我们的语料里面assistant角色的content,要去计算损失，要去做反向传播和参数优化。

###### train_full_sft.py文件的main函数参数
`out_dir`：设置输出目录
`epochs`：训练的轮次
`batch_size`：批次的大小
`learning_rate`:学习率，设置的比预训练更小
`device`：设备
`accumulation_steps`:并行计算
`warmup_iters`:梯度积累的参数
`log_interval`：日志，输出的间隔
`save_interval`:保存的周期
`dim`:向量的维度
`n_layers`:层数
`max_seq_len`:上下文窗口的长度
`use_moe`:是否使用这个混合专家模型
`data_path`:数据集的路径
我们继续看MiniMindBlock类里面的fend_forward类，前馈连接层。里面有一个隐藏层的维度。因为在全连接层里面，是把这个维度先放大，再收缩。
config.hidden_dim的计算是做了8/3的这样一个倍数方大，再做计算，是要保证这个数字是64。
下面有w1,w2,w3 三个矩阵。再从下面的线性层可以看到维度正好是先放大再缩小。
下面还有一个激活函数silu,里面正好是那三个矩阵的运算关系。
![[Pasted image 20250604220433.png]]
这个output线性层和tok_embeddings嵌入层参数是相反的，下面也有设置他们的权重是相等的，但是含义不一样，对这个嵌入层来说它实际上是一个token的一个one-hot值，转换成512的维度。
对output的每一次预测是一个512的向量维度把它映射到6400个token的一个概率值。
![[Pasted image 20250604221159.png]]
在train_full_sft.py文件。scaler是使用了梯度缩放器，用于混合精度的计算。
optimizer是设置它的优化算法是AdamW
![[Pasted image 20250604221747.png]]
调用train_epoch这个函数训练。首先设置了交叉熵损失的函数
再下面是获取数据集中的数据，把它以样本和标签以及掩码的方式加载进来。
lr参数是获取学习率
![[Pasted image 20250604221944.png]]