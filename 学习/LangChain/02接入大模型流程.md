前期准备：python>=3.11  LangChain>=0.325  langchain-deepseek>=0.1.3 (使用deepseek模型)
下载和查看版本
```bash
pip install langchain
pip show langchain 
```
![[Pasted image 20250701120054.png]]
去deepseek官网注册好API_KEY https://platform.deepseek.com/usage
然后在项目目录下新建.env文件
```.env
DEEPSEEK_API_KEY=XX
```
我们要通过`python-dotenv`库读取`env`文件中的`API_KEY`，使其加载到当前的运行环境中
```bash
pip install python-dotenv
```
使用key
```Python
import os
from dotenv import load_dotenv 
load_dotenv(override=True)

DeepSeek_API_KEY = os.getenv("DEEPSEEK_API_KEY")
print(DeepSeek_API_KEY)  # 可以通过打印查看
```
直接使用`DeepSeek`的`API`进行网络连通性测试。先下载库
```bash
pip install openai
```

```Python
from openai import OpenAI
# 初始化DeepSeek的API客户端
client = OpenAI(api_key=DeepSeek_API_KEY, base_url="https://api.deepseek.com")

# 调用DeepSeek的API，生成回答
response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "你是乐于助人的助手，请根据用户的问题给出回答"},
        {"role": "user", "content": "你好，请你介绍一下你自己。"},
    ],
)

# 打印模型最终的响应结果
print(response.choices[0].message.content)
```
![[Pasted image 20250701120305.png]]
如何可以成功返回信息，我们接下来就是让deepseek接入LangChain
首先需要安装`LangChain`的`DeepSeek`组件。这种库一般是后台使用，是隐式的
```bash
pip install langchain-deepseek
```
然后通过一个`init_chat_model`函数来初始化大模型
```Python
from langchain.chat_models import init_chat_model
model = init_chat_model(model="deepseek-chat", model_provider="deepseek")  
```
`model`用来指定要使用的模型名称（也可以是deepseek-reasoner)，而`model_provider`用来指定模型提供者，当写入`deepseek`时，会自动加载`langchain-deepseek`的依赖包，并使用在`model`中指定的模型名称用来进行交互。
```Python
question = "你好，请你介绍一下你自己。"
result = model.invoke(question)
print(result.content)
print(result)
print(result.additional_kwargs) #打印出附加信息
```
![[Pasted image 20250701120914.png]]
result返回的是AIMessage，在对话过程中我们人类发送的信息是humanmessage
工作原理
![[Pasted image 20250701112200.png]]

关于`LangChain`都支持哪些大模型以及每个模型对应的是哪个第三方依赖包，大家可以在`LangChain`的官方文档中找到，访问链接为： https://python.langchain.com/docs/integrations/chat/  里面有的模型是直接接入，后面显示的是间接接入，点进去看就知道了

LangChain接入OpenAI模型
```bash
pip install langchain-openai
```
使用流程和上面的一样

使用ollama加载本地大模型
```bashe
pip install langchain-ollama
```

```Python
from langchain_ollama import ChatOllama
```
要确保ollama已经顺利开启，并查看当前模型名称：
```bash
ollama list
```
用如下方法接入LangChain：
```Python
model = ChatOllama(model="deepseek-r1")
question = "你好，请你介绍一下你自己。"
result = model.invoke(question)
print(result.content)
```
![[Pasted image 20250701130708.png]]