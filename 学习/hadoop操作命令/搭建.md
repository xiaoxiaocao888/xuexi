#### 1.所有操作都是在liunx上完成的，首先在liunx上要可以联网
验证：ping www.baidu.com ，若是可以ping通则可以上网，上不了网的去查看“linux虚拟机联网配置.pdf” 这个文件
#### 2.修改主机名
sudo vim /etc/hosts
![[Pasted image 20241029093436.png]]
文件里面，前面部分是主机ip,后面是搭建的主机名
查看主机名：sudo vim /etc/hostname
![[Pasted image 20241029093753.png]]配置主机名的意义：后续访问采用主机名代替IP地址，需要进行域名解析器，即![[Pasted image 20241101124843.png]]需要在/etc/hosts当中配置一个映射关系，把整个集群的映射关系写进去。在集群里面，每个机器都有自己的主机名了，那么相互访问，就不要发IP地址了，直接用主机名。
#### 3.安装JDK
文件下载到本地电脑，现在是将本地电脑的文件上传到liunx


![[Pasted image 20241029095146.png]]
![[Pasted image 20241029095325.png]]
配置环境变量的目的，为了在命令行窗口下可以直接运行javac。javac将.java程序编译成.class加载到JVM转换成机器码，然后才能运行。
tar -zxvf是用于解压文件的命令，tar为打包工具，z表示调用gzip来解压文件，通常用于解压.tar.gz或.tgz格式的文件；x是解压，告诉tar从归档文件中提取文件；v是在解压过程中显示详细信息，如解压的文件名;f表示后面要紧跟文件名，即告诉tar命令要操作的文件名称
mv重命名文件或目录 mv 旧文件名 新文件名  或是mv 旧目录名 新目录名
mv移动文件或目录 mv 源文件或目录 目标目录
#### 4.安装Hadoop
第一步：下载Hadoop包
wget --no-check-certificate https://repo.huaweicloud.com/apache/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz
第二步：解压Hadoop包并转移到指定文件
tar -zxvf hadoop-3.1.3.tar.gz -C/usr/local
第三步：重命名
`[xiaoxiaocao@lqh425 ~]$ cd /usr/local
`[xiaoxiaocao@lqh425 local]$ sudo mv hadoop3.1.3 hadoop
第四步：配置
`[xiaoxiaocao@lqh425 local]$ sudo vim ~/.bashrc
`[xiaoxiaocao@lqh425 local]$ source ~/.bashrc
配置内容：
```
export HADOOP_HOME=/usr/local/hadoop
export JAVA_HOME=/usr/lib/jvm/jdk
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
```
${HADOOP_HOME}/bin为hdfs的操作指令等 
${HADOOP_HOME}/sbin为Hadoop的启停指令等

查看目录结构
![[Pasted image 20241029102454.png]]
（1）bin目录：存放对Hadoop相关服务（hdfs，yarn，mapred）进行操作的脚本
（2）etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件
（3）lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能）
（4）sbin目录：存放启动或停止Hadoop相关服务的脚本
（5）share目录：存放Hadoop的依赖jar包、文档、和官方案例
#### 5.SSH免密登录
![](file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml14324\wps1.jpg)（1）远程到自己这台虚拟机
ssh 主机名
yes
写入liunx的密码
![](file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml14324\wps2.jpg)

（2）生成密钥对、授权、修改文件权限
`[xiaoxiaocao@lqh425 hadoop]$ cd ~/.ssh/
`xiaoxiaocao@lqh425 .ssh]$ ssh-keygen -t rsa
执行完后按三次enter键盘![](file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml14324\wps3.jpg)
`[xiaoxiaocao@lqh425 .ssh]$ cat id_rsa.pub >> authorized_keys
`[xiaoxiaocao@lqh425 .ssh]$ chmod 600 ./authorized_keys

(3) 重新远程，发现无须密码，说明ssh免密设置成功
`[xiaoxiaocao@lqh425 .ssh]$ ssh lqh425
![](file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml14324\wps4.jpg)

### 集群部署
#### 单机模式
```
[atguigu@hadoophww local]$ cd /usr/local/hadoop
[atguigu@hadoophww hadoop]$ mkdir ./input
[atguigu@hadoophww hadoop]$ cp ./etc/hadoop/*.xml ./input
[atguigu@hadoophww hadoop]$ ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep ./input ./output 'dfs[a-z.]+'
[atguigu@hadoophww hadoop]$ cat ./output/*
```
先新建/input文件夹，而/output文件夹必须是不存在，程序跑完自动生成的。

#### 伪分布式
主要配置/usr/local/hadoop/etc/hadoop下面的2个文件：core-site.xml以及hdfs-site.xml
##### 1.配置core-site.xml
`[atguigu@hadoophww hadoop]$ cd /usr/local/hadoop/etc/hadoop
`[atguigu@hadoophww hadoop]$ sudo vim core-site.xml

添加如下配置信息：
```
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/local/hadoop/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoophww:9000</value>
    </property>
</configuration>
```
hadoophww为主机名
/opt/data/tmp/dfs/data
##### 2.配置hdfs-site.xml
`[atguigu@hadoophww hadoop]$ sudo vim hdfs-site.xml
```
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.namenode.http-address</name>
        <value>hadoophww:9870</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoophww:50090</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/data</value>
    </property>
</configuration>
```
##### 3.执行NameNode的格式化
`[atguigu@hadoophww hadoop]$ hdfs namenode -format
##### 4.启动集群
`[atguigu@hadoophww hadoop]$ start-all.sh
##### 5.验证节点
`[atguigu@hadoophww hadoop]$ jps
![[Pasted image 20241101122341.png]]
##### WEBUI查看
修改C:\Windows\System32\drivers\etc\hosts文件，可以用记事本打开，添加主机名和IP地址之间的映射关系（最后一行）。如下图所示：![](file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml13792\wps2.jpg)
也可以在浏览器中输入hadoophww:9870 (主机名)
![](file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml13792\wps3.jpg)
#### 分布式集群配置
NameNode和SecondaryNameNode不要安装在同一台服务器
![[Pasted image 20241101123026.png]]
##### 1.克隆
虚拟机要关机，虚拟机--管理--克隆
![[Pasted image 20241101123901.png]]
点下一页，点下一页（选择从当前状态创建），点击创建完整克隆，点下一页，
![[Pasted image 20241101124232.png]]
修改虚拟机名称，位置选择内存充足的磁盘，点击完成，就会开始克隆
![[Pasted image 20241101124323.png]]
用同样的方法克隆第二台虚拟机，克隆的虚拟机和原来的虚拟机一样，有JDK，HADOOP环境。
##### 2.修改克隆的那两台的主机名，网络配置
sudo vim /etc/hosts
查看主机名：sudo vim /etc/hostname
修改地址信息，查看“linux虚拟机联网配置.pdf” 这个文件
##### 3.配置yarn-site.xml
```
<configuration>
<!-- Site specific YARN configuration properties -->
<property>
                <name>yarn.resourcemanager.hostname</name>
                <value>hadoop102</value>
        </property>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
</configuration>
```
##### 4.配置mapred-site.xml
```
<configuration>
<property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.address</name>
                <value>hadoop100:10020</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.webapp.address</name>
                <value>hadoop100:19888</value>
        </property>
        <property>
                <name>yarn.app.mapreduce.am.env</name>
                <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
        </property>
        <property>
                <name>mapreduce.map.env</name>
                <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
        </property>
        <property>
                <name>mapreduce.reduce.env</name>
                <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
        </property>
</configuration>
```
##### 5.集群的三台虚拟机都进行hadoop集群的启动
`[atguigu@hadoophww hadoop]$ start-all.sh
##### 6.验证每台虚拟机上面的进程数
jps
![[Pasted image 20241101130025.png]]
三台虚拟机的jps是不一样的
##### 7.webui查看
##### 8.scp安全拷贝
scp可以实现服务器与服务器之间的数据拷贝。（from server1 to server2）
基本语法：scp -r $pdir/$fname $user@$host:$pdir/$fname
命令 递归 要拷贝的文件路径/名称  目的地用户@主机:目的地路径/名称
`[atguigu@hadoop100 ~]$ scp -r /etc/hosts atguigu@hadoop101:/etc/hosts
如有需要，可以选择直接用scp将内容复制到集群的其他结点上。

192.168.1.101  lqh425
192.168.1.102 Hadoop101  
192.168.1.103  hadoop 102   

![[Pasted image 20241109202853.png]]
iptables -A INPUT -p tcp --dport 22 -j ACCEPT
在INPUT链中添加一个新的规则，允许TCP协议且目标端口22的数据包进入，但是可能不是永久的