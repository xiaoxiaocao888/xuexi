1.启动docker
```
sudo systemctl start docker
sudo systemctl stop docker #暂停
sudo systemctl status docker #查看状态
```
2.验证docker是否运行
```
sudo docker ps
```
3.启动dify服务
```
cd /usr/local/bin/dify/docker
sudo docker compose up -d
sudo docker compose down #暂停
```
4.打开浏览器
```
http://localhost/install
```
5.启动ollama服务
```
ollama serve
```
##### Ollama
Ollama是一个让你能在本地运行大语言模型的工具。进入官网下载
```
https://ollama.com/download
```
更新ollama
```
curl -fsSL https://ollama.com/install.sh | sudo sh
```
![[Pasted image 20250422222908.png]]

启动ollama服务
```
ollama serve
```
打开浏览器，显示running说明启动成功
```
http://localhost:11434/
```
![[Pasted image 20250422223212.png]]

###### 安装deepseek-r1:1.5b模型
```
https://ollama.com/library/deepseek-r1:1.5b
```
复制链接里面的代码去虚拟机上执行即可
之后就会进入命令行，我们可以直接进行交互，问问题。
若是想下载其他模型，也是一样的，直接在ollama网站输入模型名称，然后下载。
![[Pasted image 20250422225143.png]]

###### 在dify页面安装ollama插件
点击右上角的设置--模型供应商--玩下滑动找到ollama--点击安装
然后就添加添加模型，
![[Pasted image 20250424174421.png]]
模型名称就是写你按照插件的名称，如deepseek-r1:11434
地址写http://192.168.1.3:11434
![[Pasted image 20250424174457.png]]

##### 宿主机IP就是虚拟机IP
vim /usr/loca/bin/dify/.env
```
APP_API_HOST=0.0.0.0      # 允许外部访问 API
APP_API_PORT=18001        # 指定 API 端口（需与 docker-compose.yml 映射一致）
MODEL_PROVIDER=ollama
OLLAMA_API_BASE_URL=http://192.168.1.3:11434 #地址是虚拟机地址
OLLAMA_REQUEST_TIMEOUT=600      # 超时时间（秒）
OLLAMA_MAX_RETRIES=5            # 失败重试次数
```